# Project 3: Continuous Integration + Deployment

## Learning Goals

- Learn how to deploy a full-stack application
- Gain hands-on experience with analysis tools, including setting up, customizing, and using them
- Practically assess and compare the costs and benefits of existing static and dynamic bug-finding tools
- Integrate CI/CD tools into development practice


## Project Context

Adding continuous integration for quality assurance is a critical part of software development. Although you have been testing your new system this whole time, you are now setting out to establish sustained practices that can be used moving forward as you iterate over and continue to improve your system.

Your manager has assigned you two major tasks. Firstly, establishing a deployment pipeline to create a test version of the website that can be sent to beta testers who have little to no experience with code development (and therefore can not set up the developer environment by themselves).

Secondly, evaluating existing tools and practices beyond simple linting or unit testing, then producing a report on the cost/benefit tradeoffs and risks of them. You will also select and integrate one (or more!) of these tools into your development process.


## Deliverables and Deadlines
There are two (2) deadlines for this project. This project is worth a total of 120 points.

**Checkpoint Deliverables** – 35 points – due Thursday, October 26th, 11:59pm

- [Deployed Application (25 pts)](#deployed-application-25-pts)
- [Tools Checkpoint (10 pts)](#tools-checkpoint-10-pts)

**Final Deliverables** – 65 points – due Thursday, November 2, 11:59pm

- [Tool Analysis Design Doc (50 pts)](#tool-analysis-design-doc-60-pts)
- [Tool Integration (15 pts)](#tool-integration-15-pts)

**Extra Credit (Individual)** - 6 points - due Thursday, November 2, 11:59pm

- [Feature Review (6 pts)](#feature-review-6-pts)

!!! info "Work Distribution"
    There are two main focuses in this project: deployment and static/dynamic analysis. For the purposes of equitable distribution of labor, we recommend that you nominate one of your members to act as the SRE for this assignment who will be primarily responsible for deployment, and have all other teammates focus on tool research and integration.


## Checkpoint Deliverables

### Deployed Application (25 pts)

Your team will be using Google Cloud Platform for the deployment of the NodeBB application. Further instructions on how to deploy can be found [here](/projects/P3/deployment).

Once you have successfully deployed your website, make sure to test within your team to ensure that your added feature(s) from Project 2 are properly integrated.

By the checkpoint deadline you should

- Submit a link to the deployed site onto Gradescope
- Add your deployed site to this [public spreadsheet](https://docs.google.com/spreadsheets/d/155eTtRrJGFE0QtTapAca3MhTSZ9qh9NlhENS4MRDN80/edit?usp=sharing), alongside your team name & UserGuide.md that your team submitted for Project 2. This will be used in [Feature Review](#feature-review-6-pts) for extra credit.

### Tools Checkpoint (10 pts)

Before jumping into tool integration, your manager would like you to research what existing analysis tools are out there that can be used with NodeBB. You will evaluate the tools, and eventually document your findings in a design document for your final deliverable.

First, identify and experiment with **at least N-1 potential static and dynamic analysis tools** that are applicable to your system, where N is the number of people in your team. We provide a [starter list of tools](#starter-list-of-tools) in the resources section below to help you get started, but you are not limited to these tools.

In your selection of tools, you should

- have **at least one** static analysis tool
- have **at least one** dynamic analysis tool
- have **at least one** tool that is not from our starter list
- **not use** any of the existing tools within NodeBB as part of your analysis (mocha/ESLint/TSLint)

For each tool that you assess

1. Create a separate testing branch in your repository (named appropriately for the tool you’re testing) to integrate the tool into your project and test out its capabilities
2. Create a pull request to the main branch from each of these testing branches. The PR should have

      - **Concrete evidence that you had successfully installed the tool** through trackable file changes demonstrating that extra files/NPM packages were installed.
      - **Artifacts that demonstrate that you have successfully run the tool on your repository.** Acceptable artifacts include output files generated by the tool, or a text file containing the terminal output from the tool; you may also attach screenshots as additional pieces of evidence. They can be attached to the Pull Request in either the description or follow-up comments.

!!! note "Grading Note"
    We will not be grading the quality or quantity of any code you put into these testing branches/PRs, just the evidence that you have successfully installed and run the tool.

In your evaluations, consider & experiment with the types of customization that are appropriate or necessary for this tool, both a priori (before they can be used in your project) and over time. Assess the strengths and weaknesses of each tool/technique, both quantitatively and qualitatively.

Consider some of the following questions:

- What types of problems are you hoping your tooling will catch?  What types of problems does this particular tool catch?
- What types of customization are possible or necessary?
- How can/should this tool be integrated into a development process?
- Are there many false positives? False negatives? True positive reports about things you don’t care about?

!!! info "Tool Evaluation"
    There are a lot of different factors to consider when evaluating a tool. We recommend discussing with your teammates and deciding on a group of metrics to focus on when performing evaluations.

!!! note "Time Management"
    Don’t spend too long for this checkpoint. Set deadlines within your team to ensure that you have enough time for both the design document and integration deliverables described below for the final deadline.

By the checkpoint deadline, your team will submit

- your **initial list of the N-1 tools** that your team plans on exploring, and
- links to the **PRs that demonstrate that you have successfully installed and run each of these tools**

## Final Deliverables

### Tool Analysis Design Doc (50 pts)

Create a Design Document/RFC that includes:

- A **tools evaluations section** detailing your team’s analysis on each of the tools you experimented with
- A **justification section** explaining which tool(s) you think the project should use moving forward
- An **integration section** describing how the selected tool(s) shall be integrated into your process
- A **conclusion section** summarizing your work

Below, we provide more detailed instructions and page limit recommendations for each of the sections.

#### Tool Evaluations (~N pages)

For each of the N-1 tools explored by your team, you must provide:

- Name and high-level description of what the tool does and a link to its documentation/source
- Whether the tool is used for static or dynamic analysis
- A link to the pull request made from the testing branch for this tool
- A pro/con analysis of the tool and appropriate evidence in the form of screenshots to support your claims. You can use the questions provided in the research you did for the checkpoint to shape your analysis.

Each of these sections should take up approximately 1 page (including screenshots) and no more than 2 pages.

#### Justification (~Half a page)

After going through each of the tools, you should explicitly state the tool(s) you are choosing to integrate for this project and provide a justification for why you are selecting this tool. You should refer to the pro/con analysis done in the prior sections and how they align with the goals of your team and the project overall.

You **must recommend at least one tool**, even if it’s with reservations.

#### Integration (<1 page)

This section should address the different factors to take into consideration when integrating a new tool. At minimum, you should address the following:

- **Technical Questions**
    - How are you integrating the tool (high-level)? At what point in the development/deployment process shall it be integrated? What sorts of customization or configuration will you be using?
    - If you added any specific configuration to allow the main branch of your repository to pass its status checks, add the justification for those decisions in this section.
- **Social Integration Questions**
    - How do you foresee the team using the tool during their development process? Consider the incentives & deterrents to the developers when it comes to using the tool, and their personal motivation to use it.

Your answers should be based on your experiences running the tools on your team repository and be grounded in data from your research on different factors such as tool usability, output, and customizability.

Keep this section updated as you work on implementing the integration.

#### Conclusion (<1 page)

In this section, provide a brief summary of your findings along with items that were not addressed in the previous sections.

- Are there any open questions?
- Are there any issues you consider to be out of scope?
